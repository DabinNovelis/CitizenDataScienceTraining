{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b73a47b8-691b-411b-ac76-6e8a5e2b30c1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Class 9 - Part 1 \n",
    "### Advanced Machine Learning Concepts\n",
    "In this practical session, we will delve into several advanced machine learning concepts crucial for developing robust predictive models. Here's an overview of the key topics we will explore:\n",
    "\n",
    "1. **Curse of Dimensionality:** We will explore how high-dimensional spaces increase the sparsity of data, making it difficult to effectively train and optimize machine learning models due to increased computational complexity and data requirements.\n",
    "2. **Linear Discriminant Analysis (LDA):** This topic covers LDA, a dimensionality reduction technique that is also used for classification. It seeks to maximize the separability among known categories.\n",
    "3. **Hyperparameter Tuning:** We'll discuss methods to select the best set of hyperparameters for a given machine learning model, enhancing its performance on unseen data.\n",
    "   - **Grid Search:** This is a technique for hyperparameter tuning that methodically builds and evaluates a model for each combination of algorithm parameters specified in a grid.\n",
    "4. **Metrics in Machine Learning:** We will review various metrics used to evaluate the performance of machine learning models, such as accuracy, precision, recall, F1-score, and AUC-ROC among others.\n",
    "   - **Confusion Matrix:** An important tool to summarize the performance of a classification algorithm. It provides insights into the types of errors made by the model.\n",
    "5. **k-Fold Cross-Validation:** This topic covers the technique of dividing the dataset into k-subsets and using each in turn for testing a model trained on the remaining k-1 subsets. It is a method used to estimate the skill of the model on new data.\n",
    "\n",
    "This session will provide hands-on experience in implementing these concepts in Python, using various libraries to help illustrate these advanced techniques in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e13ba69e-1ce9-489c-982e-01b827efcece",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries and modules for data manipulation, machine learning, and metric evaluation:\n",
    "import numpy as np  # NumPy is used for numerical operations and handling arrays.\n",
    "import matplotlib.pyplot as plt  # Matplotlib is used for creating static, interactive, and animated visualizations in Python.\n",
    "import seaborn as sns  # Seaborn is a Python data visualization library based on matplotlib that provides a high-level interface for drawing attractive and informative statistical graphics.\n",
    "import pandas as pd  # Pandas is a fast, powerful, flexible, and easy-to-use open-source data analysis and manipulation tool, built on top of the Python programming language.\n",
    "\n",
    "# Import specific functions and classes from scikit-learn (sklearn):\n",
    "from sklearn.datasets import fetch_lfw_people  # Function to load the Labeled Faces in the Wild (LFW) people dataset.\n",
    "from sklearn.model_selection import train_test_split  # Function to split datasets into training and testing subsets.\n",
    "from sklearn.preprocessing import StandardScaler  # Class for standardizing features by removing the mean and scaling to unit variance.\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis  # A classifier with a linear decision boundary.\n",
    "from sklearn.neighbors import KNeighborsClassifier  # Classifier implementing the k-nearest neighbors vote.\n",
    "from sklearn.svm import SVC  # Support Vector Machine classifier for both classification and regression analysis.\n",
    "from sklearn.naive_bayes import GaussianNB  # Classifier implementing Naive Bayes algorithm for assuming Gaussian distributed features.\n",
    "from sklearn.metrics import accuracy_score  # Function to calculate the accuracy, the set of labels predicted for a sample must match the corresponding set of labels in y_true.\n",
    "from sklearn.model_selection import GridSearchCV # GridSearchCV is a tool that helps in tuning hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "487711aa-17e8-404b-a619-1d44d9d8c615",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Fetch the Labeled Faces in the Wild (LFW) dataset, specifying that only those people should be included\n",
    "# who have at least 70 images in the dataset. This helps ensure a sufficiently large number of samples\n",
    "# for each class, which is important for training stable machine learning models. The images are resized\n",
    "# to 0.4 of their original size to reduce computational cost and improve processing speed.\n",
    "lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n",
    "\n",
    "# Extract feature data from the dataset; this typically includes pixel values of the images, \n",
    "# which serve as the input features for machine learning models.\n",
    "X = lfw_people.data\n",
    "\n",
    "# Extract the target labels corresponding to the identities of the people in the images.\n",
    "# These labels are used as the output or target variable for supervised learning models.\n",
    "y = lfw_people.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc1cb490-859f-4d2f-93c5-923ef52c1784",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Data Description:\n",
    "The Labeled Faces in the Wild (LFW) People dataset from scikit-learn is designed for studying face recognition problems. It's preprocessed to facilitate direct usage in machine learning models, making it a popular choice for image-based classification tasks.\n",
    "- **Features:** Each image is preprocessed and flattened into a one-dimensional array. The dataset typically involves images that have been resized, converted to grayscale, and each pixel's intensity is used as a feature. The number of features per sample depends on the image size specified during loading; for instance, if images are resized to 0.4 of their original size, this significantly reduces the dimensionality.\n",
    "- **Target:** The target variable is the identity of the person in the photograph. It's a multiclass classification problem, with each class representing a different individual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7305100-b07a-40cd-a506-2f346240be34",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Retrieve the names corresponding to the target labels in the LFW dataset, which are essentially the names\n",
    "# of the individuals whose images are included in the dataset. These names are useful for understanding\n",
    "# and interpreting the target labels.\n",
    "target_names = lfw_people.target_names\n",
    "\n",
    "# Print some details about the dataset to understand its composition and structure:\n",
    "print(\"- Number of samples: \", X.shape[0])  # The total number of images in the dataset.\n",
    "print(\"- Number of features per sample (flattened image size): \", X.shape[1])  # The number of pixels in each image, representing features.\n",
    "print(\"- Number of classes (individuals): \", len(target_names))  # The number of different individuals, i.e., the class labels.\n",
    "print(\"- Feature names (not applicable as features are pixel values): \", \"Pixel values from resized images\")  # In this dataset, features are raw pixel values, so there are no 'feature names' as such.\n",
    "print(\"- Target names (individuals): \", target_names)  # Names of the individuals, which correspond to the class labels in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6bd8740-ac4c-4ac2-9473-c3cb3ca0550e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize an instance of StandardScaler, which standardizes features by removing the mean and scaling to unit variance.\n",
    "# This is an important preprocessing step for many machine learning algorithms to perform optimally, especially for those\n",
    "# that assume data is normally distributed or those that are sensitive to the scale of the features like SVM and KNN.\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the data and then transform it. This means the scaler computes the mean and standard deviation of each feature\n",
    "# in the dataset to be used for later scaling (subtracting the mean and dividing by the standard deviation), and then it transforms\n",
    "# the data to put it onto one scale.\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28381d28-59dc-486e-a63d-078e5f439b68",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Split the standardized feature data and corresponding labels into training and testing sets.\n",
    "# 'X_scaled' contains the scaled pixel values of the images, and 'y' contains the target labels (identities of individuals).\n",
    "# The data is split with 30% reserved for the test set to evaluate the model's performance on unseen data.\n",
    "# The 'random_state' parameter ensures that the split is reproducible, meaning the same training and testing sets\n",
    "# are generated each time the code is run, which is important for debugging and comparing model performance across different runs.\n",
    "X_train_scaled, X_test_scaled, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c59a1173-57ad-4aa9-bc2d-80e90dfdee47",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define a dictionary of classifiers for easy reference and batch processing. \n",
    "# This dictionary associates each classifier type with its corresponding instance:\n",
    "classifiers = {\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=3), # k-Nearest Neighbors classifier initialized with 3 nearest neighbors. \n",
    "                                                #Useful for non-linear data.\n",
    "    'SVM': SVC(kernel='linear'),                # Support Vector Machine classifier with a linear kernel. \n",
    "                                                # Good for linearly separable data.\n",
    "    'Naive Bayes': GaussianNB()                 # Naive Bayes classifier assuming Gaussian distribution of features. \n",
    "                                                # Effective for large feature spaces.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaf558b2-df36-4d20-b0cd-f5e6aae26bef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary to store the accuracy results of each classifier when trained on scaled data (Original).\n",
    "results_scaled = {}\n",
    "\n",
    "# Loop through each classifier defined in the 'classifiers' dictionary.\n",
    "for name, clf in classifiers.items():\n",
    "    # Train the classifier on the scaled training data.\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predict the labels of the test dataset using the trained classifier.\n",
    "    predictions_scaled = clf.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate the accuracy by comparing the predicted labels to the actual labels of the test data.\n",
    "    accuracy_scaled = accuracy_score(y_test, predictions_scaled)\n",
    "    \n",
    "    # Store the accuracy of each classifier in the 'results_scaled' dictionary, keyed by the classifier's name.\n",
    "    results_scaled[name] = accuracy_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f8ac7bd-03f6-473f-9fce-c35d016d311f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Print out the accuracy results for each classifier trained on scaled data(Original).\n",
    "print(\"Accuracy using Scaled Data (Original):\")\n",
    "for name, acc in results_scaled.items():\n",
    "  # Print each classifier's name and its corresponding accuracy formatted to two decimal places.\n",
    "    print(f\"{name}: {acc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "704320d1-f008-4ad6-87af-7495b4235d4a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize an instance of Linear Discriminant Analysis (LDA). LDA is a dimensionality reduction technique \n",
    "# that is also commonly used as a linear classifier. It projects the data onto a lower-dimensional space \n",
    "# with good class separability in order to maximize the ratio of between-class variance to within-class variance.\n",
    "\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "\n",
    "# Fit the LDA model to the scaled training data and transform the training data to reduce its dimensionality.\n",
    "# This step involves finding the axes that maximize the separation between multiple classes and using these axes \n",
    "# to project the data into a space with fewer dimensions.\n",
    "X_train_lda = lda.fit_transform(X_train_scaled, y_train)\n",
    "\n",
    "# Transform the scaled test data using the same LDA model. Note that we only transform the test data \n",
    "# (without fitting) because the transformation must use the model parameters learned from the training data.\n",
    "# This ensures that the test data is projected in the same way as the training data.\n",
    "X_test_lda = lda.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a54117e2-9f3c-4092-89fe-8646e4e4ff37",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary to store the accuracy results of each classifier when trained and tested on LDA-transformed data.\n",
    "results_lda = {}\n",
    "\n",
    "# Loop through the classifiers defined in the 'classifiers' dictionary.\n",
    "for name, clf in classifiers.items():\n",
    "    # Train each classifier on the LDA-transformed training data.\n",
    "    clf.fit(X_train_lda, y_train)\n",
    "    \n",
    "    # Use the trained classifier to make predictions on the LDA-transformed test data.\n",
    "    predictions_lda = clf.predict(X_test_lda)\n",
    "    \n",
    "    # Calculate the accuracy of the predictions by comparing them to the true labels of the test data.\n",
    "    accuracy_lda = accuracy_score(y_test, predictions_lda)\n",
    "    \n",
    "    # Store the accuracy of each classifier in the 'results_lda' dictionary, keyed by the classifier name.\n",
    "    results_lda[name] = accuracy_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2ad7b2f-cc09-47c6-bf41-ab50057b8a36",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Print out the accuracy results for each classifier trained on scaled data (Original).\n",
    "print(\"Accuracy using Scaled Data (Original):\")\n",
    "for name, acc in results_scaled.items():\n",
    "  # Print each classifier's name and its corresponding accuracy formatted to two decimal places.\n",
    "    print(f\"{name}: {acc:.2f}\")\n",
    "# Print out the accuracy results for LDA-Transformed data.\n",
    "print(\"\\nAccuracy using LDA-Transformed Data:\")\n",
    "for name, acc in results_lda.items():\n",
    "  # Print each classifier's name and its corresponding accuracy formatted to two decimal places.\n",
    "    print(f\"{name}: {acc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ebaebc1a-da0e-4e4d-aa7c-24bfa4838765",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Analysis of the Results\n",
    "- **K-Nearest Neighbors (KNN):** The accuracy significantly improves when using LDA. This suggests that LDA is effective in reducing the dimensions while enhancing the separability of the classes for KNN, which struggles with high-dimensional data due to the curse of dimensionality.\n",
    "- **Support Vector Machine (SVM):** SVM performs better on the original scaled data compared to the LDA-transformed data. SVM is generally effective in high-dimensional spaces, especially with appropriate kernel choices, and might lose valuable information if the data is overly simplified or if important discriminative features are lost during transformation.\n",
    "- **Naive Bayes:** There is a substantial increase in accuracy with LDA. Naive Bayes often benefits from LDA because LDA can help alleviate some of the independence assumptions of Naive Bayes by projecting features into a space where class separability is maximized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec67624a-c972-4350-ac10-2e114b7f86c0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Grid Search: \n",
    "To fine-tune the hyperparameters of the K-Nearest Neighbors (KNN), Support Vector Machine (SVM), and Naive Bayes classifiers using Grid Search, and focusing only on the LDA-transformed data, you can use GridSearchCV from scikit-learn. This tool allows you to define a grid of hyperparameter ranges and automatically finds the best combination of parameters through cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56a30af4-4f90-401e-8350-1dd2f7911e54",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define parameter grid for KNN classifier tuning. This dictionary specifies multiple hyperparameters to explore:\n",
    "knn_params = {\n",
    "    'n_neighbors': range(10, 30),  # Exploring a broader range of neighbors, specifically between 10 and 30.\n",
    "    'weights': ['uniform', 'distance'],  # Testing both possible weighting strategies: uniform and distance-based.\n",
    "    'metric': ['euclidean', 'manhattan', 'chebyshev'],  # Including three types of distance metrics to evaluate which performs best.\n",
    "    'p': [1, 2]  # Defining 'p' parameter values for the Minkowski metric: 1 for Manhattan and 2 for Euclidean distances.\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV with the KNN classifier. Set the defined parameter grid, use 5-fold cross-validation,\n",
    "# set scoring to 'accuracy' to evaluate model performance, and enable verbose output for more detailed logging during execution.\n",
    "knn_grid = GridSearchCV(KNeighborsClassifier(), knn_params, cv=5, scoring='accuracy', verbose=1)\n",
    "\n",
    "# Fit the grid search model to the LDA-transformed training data to find the best hyperparameters.\n",
    "knn_grid.fit(X_train_lda, y_train)\n",
    "\n",
    "# Print the best hyperparameters found during the grid search.\n",
    "print(f\"Best parameters for KNN: {knn_grid.best_params_}\")\n",
    "\n",
    "# Print the highest cross-validated accuracy achieved with the best hyperparameters.\n",
    "print(f\"Best cross-validated accuracy for KNN: {knn_grid.best_score_:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31fa5ccd-852d-448f-90fd-4176a159383e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define parameter grid for SVM classifier tuning. The grid specifies a range of values to be tested for each hyperparameter:\n",
    "svm_params = {\n",
    "    'C': [0.01, 0.1, 1],  # Testing regularization parameters to understand the effect of slightly larger and smaller values around the previously effective ones.\n",
    "    'kernel': ['linear'],  # Focusing on the linear kernel as it was previously found to be effective.\n",
    "    'gamma': [0.1, 1, 3.6, 10],  # Exploring values around 3.6 to fine-tune the model's fit.\n",
    "    'degree': [3]  # Including degree for completeness, though it's not used by the linear kernel, it's required by the API.\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV with the SVM classifier. Specify the parameter grid, use 5-fold cross-validation,\n",
    "# set scoring to 'accuracy' to evaluate model performance, and enable verbose output for progress updates during execution.\n",
    "svm_grid = GridSearchCV(SVC(), svm_params, cv=5, scoring='accuracy', verbose=1)\n",
    "\n",
    "# Fit the grid search model to the LDA-transformed training data to find the best hyperparameters.\n",
    "svm_grid.fit(X_train_lda, y_train)\n",
    "\n",
    "# Print the best hyperparameters that grid search found for the SVM classifier.\n",
    "print(f\"Best parameters for SVM: {svm_grid.best_params_}\")\n",
    "\n",
    "# Print the highest cross-validated accuracy achieved with these best hyperparameters.\n",
    "print(f\"Best cross-validated accuracy for SVM: {svm_grid.best_score_:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a7493a6-e673-4c86-aa4f-abecfad09e5b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define parameter grid for Naive Bayes classifier tuning. The grid focuses on 'var_smoothing',\n",
    "# which adjusts the variance part of the calculation to prevent overfitting on very small data samples.\n",
    "# Here, 50 values are linearly spaced between 0.045 and 0.055 to finely tune this parameter.\n",
    "nb_params = {\n",
    "    'var_smoothing': np.linspace(0.045, 0.055, 50)\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV with GaussianNB as the classifier model, specifying the parameter grid,\n",
    "# number of folds for cross-validation (cv=5), the scoring metric as 'accuracy', and verbosity level 1\n",
    "# for progress updates during the model fitting process.\n",
    "nb_grid = GridSearchCV(GaussianNB(), nb_params, cv=5, scoring='accuracy', verbose=1)\n",
    "\n",
    "# Fit the GridSearchCV object to the LDA-transformed training data along with the target labels.\n",
    "nb_grid.fit(X_train_lda, y_train)\n",
    "\n",
    "# After the grid search completes, print the best hyperparameters found and the highest\n",
    "# cross-validated accuracy achieved with those parameters.\n",
    "print(f\"Best parameters for Naive Bayes: {nb_grid.best_params_}\")\n",
    "print(f\"Best cross-validated accuracy for Naive Bayes: {nb_grid.best_score_:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71dbf810-c55c-44b9-9ce3-dfdc5127545f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Print out the accuracy results for LDA-Transformed data.\n",
    "print(\"Accuracy using LDA-Transformed Data:\")\n",
    "for name, acc in results_lda.items():\n",
    "  # Print each classifier's name and its corresponding accuracy formatted to two decimal places.\n",
    "    print(f\"{name}: {acc:.2f}\")\n",
    "\n",
    "# Retrain KNN with the best hyperparameters found\n",
    "knn_optimized = KNeighborsClassifier(**knn_grid.best_params_)\n",
    "knn_optimized.fit(X_train_lda, y_train)\n",
    "knn_optimized_accuracy = accuracy_score(y_test, knn_optimized.predict(X_test_lda))\n",
    "print(f\"\\nOptimized KNN Accuracy: {knn_optimized_accuracy:.2f}\")\n",
    "\n",
    "# Retrain SVM with the best hyperparameters found\n",
    "svm_optimized = SVC(**svm_grid.best_params_)\n",
    "svm_optimized.fit(X_train_lda, y_train)\n",
    "svm_optimized_accuracy = accuracy_score(y_test, svm_optimized.predict(X_test_lda))\n",
    "print(f\"Optimized SVM Accuracy: {svm_optimized_accuracy:.2f}\")\n",
    "\n",
    "# Retrain Naive Bayes with the best hyperparameters found\n",
    "nb_optimized = GaussianNB(**nb_grid.best_params_)\n",
    "nb_optimized.fit(X_train_lda, y_train)\n",
    "nb_optimized_accuracy = accuracy_score(y_test, nb_optimized.predict(X_test_lda))\n",
    "print(f\"Optimized Naive Bayes Accuracy: {nb_optimized_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37f5d186-ed19-4661-8d8e-64979320e5c0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "Author: <b>Julio Iglesias</b>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Class_9_Supervised_Learning-Student_Part1",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
