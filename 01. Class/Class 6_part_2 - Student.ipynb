{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97705e7c-638a-4d00-a779-fd4f0860d539",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Very often we've to deal with datasets that aren't ready to be analyzed and modelled. We then need to apply feature engineering techniques on the raw data in order to have an appropriate dataset for performing advanced analysis. We've already talked about techniques to deal with *missing values* and *outliers* and how to perform *discretization*. Now, we'll learn about other feature engineering techniques such as:\n",
    "- Normalization\n",
    "- Standardization\n",
    "- Transformation from categorical to dummy variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97c73d8c-fbf4-40da-9033-3921008bd07b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Feature Scaling\n",
    "\n",
    "**Feature scaling** refers to the methods or techniques used to normalize the range of independent variables in our data, or in other words, the methods to set the feature value range within a similar scale. Feature scaling is generally the last step in the data preprocessing pipeline, performed **just before training the machine learning algorithms**.\n",
    "\n",
    "**Why is it important?**\n",
    "\n",
    "- The regression coefficients of linear models are directly influenced by the scale of the variable.\n",
    "- Variables with bigger magnitude / larger value range dominate over those with smaller magnitude / value range\n",
    "- Gradient descent converges faster when features are on similar scales\n",
    "- Feature scaling helps decrease the time to find support vectors for SVMs\n",
    "- Euclidean distances are sensitive to feature magnitude.\n",
    "- Some algorithms, like PCA require the features to be centered at 0.\n",
    "\n",
    "**Techniques:**\n",
    "\n",
    "There are different techniques, such as \n",
    "- Standardization\n",
    "- Mean normalization\n",
    "- Scaling to minimum and maximum values - MinMaxScaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f064de1d-bd67-4f30-a65d-0cbb9815624c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Standardization\n",
    "\n",
    "Standardization involves centering the variable at zero, and standardizing the variance to 1. The procedure involves subtracting the mean of each observation and then dividing by the standard deviation:\n",
    "\n",
    "**z = (x - x_mean) /  std**\n",
    "\n",
    "The result of the above transformation is **z**, which is called the z-score, and represents how many standard deviations a given observation deviates from the mean. A z-score specifies the location of the observation within a distribution (in numbers of standard deviations respect to the mean of the distribution). The sign of the z-score (+ or - ) indicates whether the observation is above (+) or below ( - ) the mean.\n",
    "\n",
    "The shape of a standardized (or z-scored normalised) distribution will be identical to the original distribution of the variable. In other words, **standardizing a variable does not normalize the distribution of the data**.\n",
    "\n",
    "In a nutshell, standardization:\n",
    "\n",
    "- centers the mean at 0\n",
    "- scales the variance at 1\n",
    "- preserves the shape of the original distribution\n",
    "- the minimum and maximum values of the different variables may vary\n",
    "- preserves outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17e5def6-de9b-40b7-b236-b22b2116a1a1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's use a dataset from Scikit-learn about Boston House Prices to learn how to do it. The dataset has characteristics about houses and the neighborhoods and can be used to predict the house price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47c283c8-30e8-4dd9-8ca1-b1813a147185",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f39ada5c-8978-4f17-82b6-6dc32975ffcd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# the scaler - for standardization\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba8d6cde-75eb-442d-b505-ba50fd013e7e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('/dbfs/FileStore/CDS2024/boston_house_prices.csv', header=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c80d2ee1-b39d-4029-a8c8-8d42720bafb7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97399331-02bf-49d0-800c-03385d99a10d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Note how the variables show different magnitudes or scales. In other words, how **the mean values are not centered at zero, and the standard deviations are not scaled to 1**.\n",
    "\n",
    "When standardizing the data set, we need to first identify the **mean and standard deviation** of the variables. These parameters need to be learned from the train set, stored, and then used to scale test and future data. So let's split the dataset as we've done before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f83a68ef-f9e8-4ba8-ad17-687e52547e1e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# let's separate the data into training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.drop('MEDV', axis=1),\n",
    "                                                    data['MEDV'],\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "726f2985-097e-4366-bc2b-3a20cf074468",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Standardization\n",
    "\n",
    "The StandardScaler from scikit-learn removes the mean and scales the data to unit variance. Plus, it learns and stores the parameters needed for scaling. Thus, it is top choice for this feature scaling technique.\n",
    "\n",
    "On the downside, you can't select which variables to scale directly, it will scale the entire data set, and it returns a NumPy array, without the variable values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "946800f3-7262-4e4f-b04f-286033fb9e7e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# set up the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# fit the scaler to the train set, it will learn the parameters\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# transform train and test sets\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# the scaler stores the mean and standard deviation of the features, learned from train set\n",
    "print(scaler.mean_)\n",
    "print(scaler.scale_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b558b152-2988-49e3-bb6b-706ad6220858",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# let's transform the returned NumPy arrays to dataframes\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "\n",
    "X_train_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ff7502b-1d0f-408b-82fa-6f6bb70adcf3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# let's have a look at the original training dataset: mean and standard deviation\n",
    "# np.round to reduce the number of decimals plates to 1\n",
    "np.round(X_train.describe(), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ca54615-6e98-42e8-a1e4-341014b47b79",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# let's have a look at the scaled training dataset: mean and standard deviation\n",
    "# np.round to reduce the number of decimals plates to 1\n",
    "np.round(X_train_scaled.describe(), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fc2b3d8-f591-41d4-9ba2-21a76695fa1d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "As expected, the mean of each variable, which were not centered at zero, is now around zero and the standard deviation is set to 1. Note however, that the minimum and maximum values vary according to how spread the variable was to begin with and is highly influenced by the presence of outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ef23d37-0d64-443c-935b-3c7399b360aa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's compare the variable distributions before and after scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a86d9ec-e269-4e2d-8308-0a1b1f5f0910",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 5))\n",
    "\n",
    "# before scaling\n",
    "ax1.set_title('Before Scaling')\n",
    "sns.kdeplot(X_train['RM'], ax=ax1)\n",
    "sns.kdeplot(X_train['LSTAT'], ax=ax1)\n",
    "sns.kdeplot(X_train['CRIM'], ax=ax1)\n",
    "\n",
    "# after scaling\n",
    "ax2.set_title('After Standard Scaling')\n",
    "sns.kdeplot(X_train_scaled['RM'], ax=ax2, label = 'RM')\n",
    "sns.kdeplot(X_train_scaled['LSTAT'], ax=ax2, label = 'LSTAT')\n",
    "sns.kdeplot(X_train_scaled['CRIM'], ax=ax2, label = 'CRIM')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6798f9a-3d7d-4d66-9f2a-c5eb537b7384",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Note from the above plots how standardization centered all the distributions at zero, but it preserved their original distribution. The value range is not identical, but it looks more homogeneous across the variables. \n",
    "\n",
    "Note something interesting in the following plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "131f4df7-4a4d-4444-b6c2-e4c72f74b976",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# let's compare the variable distributions before and after scaling\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 5))\n",
    "\n",
    "# before scaling\n",
    "ax1.set_title('Before Scaling')\n",
    "sns.kdeplot(X_train['AGE'], ax=ax1)\n",
    "sns.kdeplot(X_train['DIS'], ax=ax1)\n",
    "sns.kdeplot(X_train['NOX'], ax=ax1)\n",
    "\n",
    "# after scaling\n",
    "ax2.set_title('After Standard Scaling')\n",
    "sns.kdeplot(X_train_scaled['AGE'], label = 'Age', ax=ax2)\n",
    "sns.kdeplot(X_train_scaled['DIS'], label = 'Dis', ax=ax2)\n",
    "sns.kdeplot(X_train_scaled['NOX'], label = 'Nox', ax=ax2)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "480c854d-6acf-4c68-88dc-49d84abe000d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In the above plot, we can see how, by scaling, the variable NOX, which varied across a very narrow range of values [0-1], and AGE which varied across [0-100], now spread over a more homogeneous range of values, so that we can compare them directly in one plot, whereas before it was difficult. In a linear model, AGE would dominate the output, but after standardization, both variables will be able to have an input (assuming that they are both predictive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af42565d-3c77-4a21-b286-f02b3588e63c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(X_train['AGE'], X_train['NOX'])\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Nox\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f467730b-3163-4d30-95f3-4fdf3c1ad1b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(X_train_scaled['AGE'], X_train_scaled['NOX'])\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Nox\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "293e5bd6-e111-4daf-80bc-afee6b5608a9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Mean Normalization\n",
    "Mean normalization involves centering the variable at zero, and re-scaling to the value range. The procedure involves subtracting the mean of each observation and then dividing by difference between the minimum and maximum value:\n",
    "\n",
    "**x_scaled = (x - x_mean) / ( x_max - x_min)**\n",
    "\n",
    "\n",
    "The result of the above transformation is a distribution that is centered at 0, and its minimum and maximum values are within the range of -1 to 1. The shape of a mean normalized distribution will be very similar to the original distribution of the variable, but the variance may change, so not identical.\n",
    "\n",
    "Again, this technique will not **normalize the distribution of the data**.\n",
    "\n",
    "In a nutshell, mean normalization:\n",
    "\n",
    "- centers the mean at 0\n",
    "- variance will be different\n",
    "- may alter the shape of the original distribution\n",
    "- the minimum and maximum values squeezed between -1 and 1\n",
    "- preserves outliers\n",
    "\n",
    "Obs: There is no Scikit-learn transformer for mean normalization, but we can implement it using a combination of 2 other transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f28ba124-1c74-4068-8176-3b27acc54bb4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's see again the statistical parameters of the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e184424-881b-46cc-87f4-4c96ff3ac939",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aab3b435-8fac-48b6-8ef7-559a2c53a068",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Note for this demo, how **the mean values are not centered at zero, and the min and max value vary across a big range**.\n",
    "\n",
    "We need to first identify **the mean, the minimum and maximum values** of the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd8d6335-f6cf-4334-9bda-b869c710c5a4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "As we did before, let's separate into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc0685c4-7f10-4aa6-a2e8-368239932fea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data.drop('MEDV', axis=1),\n",
    "                                                    data['MEDV'],\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09504994-9fdb-41b9-ab46-9cd8d090755c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Mean normalization with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd4d5e9e-e8c5-4f2f-adfe-659e9101e4e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "means = X_train.mean(axis=0)\n",
    "ranges = X_train.max(axis=0)-X_train.min(axis=0)\n",
    "means, ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce0095ec-4c67-470e-88b7-f7647d16b1da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_train_scaled = (X_train - means) / ranges\n",
    "X_test_scaled = (X_test - means) / ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d534f9b5-f66b-4d99-885f-a93024bd69e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "np.round(X_train_scaled.describe(), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96e8fee8-1847-4c68-8844-886397859b49",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "As expected, the mean of each variable, which were not centered at zero, is now around zero and the min and max values vary approximately between -1 and 1. Note however, that the standard deviations vary according to how spread the variable was to begin with and is highly influenced by the presence of outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c957221-23ae-4f40-aae8-6935d5136c90",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's compare the variable distributions before and after scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "765c64ec-8d06-433e-a285-003b05bfb4af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# let's compare the variable distributions before and after scaling\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 5))\n",
    "\n",
    "# before scaling\n",
    "ax1.set_title('Before Scaling')\n",
    "sns.kdeplot(X_train['RM'], ax=ax1)\n",
    "sns.kdeplot(X_train['LSTAT'], ax=ax1)\n",
    "sns.kdeplot(X_train['CRIM'], ax=ax1)\n",
    "\n",
    "# after scaling\n",
    "ax2.set_title('After Mean Normalisation')\n",
    "sns.kdeplot(X_train_scaled['RM'], label = 'RM', ax=ax2)\n",
    "sns.kdeplot(X_train_scaled['LSTAT'], label = 'LSTAT', ax=ax2)\n",
    "sns.kdeplot(X_train_scaled['CRIM'], label = 'CRIM', ax=ax2)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2260904-9223-4fbc-967a-529cb42273d2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "As we can see the main effect of mean normalization was to center all the distributions at zero, and the values range it between -1 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a3b5469-eb88-4f10-9599-27c422e5c10c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Scaling to Minimum and Maximum values - MinMaxScaling\n",
    "\n",
    "Minimum and maximum scaling squeezes the values between 0 and 1. It subtracts the minimum value from all the observations, and then divides it by the value range:\n",
    "\n",
    "X_scaled = (X - X.min / (X.max - X.min)\n",
    "\n",
    "\n",
    "The result of the above transformation is a distribution which values vary within the range of 0 to 1. But the mean is not centered at zero and the standard deviation varies across variables. The shape of a min-max scaled distribution will be similar to the original variable, but the variance may change, so not identical. This scaling technique is also sensitive to outliers.\n",
    "\n",
    "As we said before, this technique will not **normalize the distribution of the data**.\n",
    "\n",
    "In a nutshell, MinMaxScaling:\n",
    "\n",
    "- does not center the mean at 0\n",
    "- variance varies across variables\n",
    "- may not preserve the shape of the original distribution\n",
    "- the minimum and maximum values are 0 and 1.\n",
    "- sensitive outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee20d318-7a13-420e-8ad7-6135b96d95e4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bfb3f35-1ad1-41e9-9a9c-0dec3745d141",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's remember the statistical parameters of the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0c73fb9-5139-477f-a8cd-345b2cd07616",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "698f70d4-a11a-44f3-82a1-e59121907f57",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Note how **the minimum and maximum values are quite different for different variables**.\n",
    "\n",
    "Following the same logic as before, we need to first identify the minimum and maximum values of the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5661d79f-3281-473e-be9c-0d3147776ac9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# let's separate the data into training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.drop('MEDV', axis=1),\n",
    "                                                    data['MEDV'],\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "733f7822-e60f-43e0-8aa5-db9a612ea084",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# set up the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# fit the scaler to the train set, it will learn the parameters\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# transform train and test sets\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30cace0e-0be7-4f33-9b30-ffca9b42f9ce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(scaler.data_max_)\n",
    "print(scaler.min_)\n",
    "print(scaler.data_range_)\n",
    "#range = max - min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "703304e9-6dec-4d53-9b96-14e0b6bf8b1e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# let's transform the returned NumPy arrays to dataframe\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ecd044a-a6a4-42b7-a3a4-cb420b9f1b4c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "np.round(X_train_scaled.describe(), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fccafd8a-1c40-4b65-9f1f-7641b8e5aea9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "As expected, the minimum and maximum values for all the variables are 0 and 1, respectively. The mean is not centered at zero, and the variance changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c80bba47-0425-4766-80ac-d7d4eba796a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# let's compare the variable distributions before and after scaling\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 5))\n",
    "\n",
    "# before scaling\n",
    "ax1.set_title('Before Scaling')\n",
    "sns.kdeplot(X_train['RM'], ax=ax1)\n",
    "sns.kdeplot(X_train['LSTAT'], ax=ax1)\n",
    "sns.kdeplot(X_train['CRIM'], ax=ax1)\n",
    "\n",
    "# after scaling\n",
    "ax2.set_title('After Min-Max Scaling')\n",
    "sns.kdeplot(X_train_scaled['RM'], label = 'RM', ax=ax2)\n",
    "sns.kdeplot(X_train_scaled['LSTAT'], label = 'LSTAT', ax=ax2)\n",
    "sns.kdeplot(X_train_scaled['CRIM'], label = 'CRIM', ax=ax2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c268216e-f9da-47ce-bdcc-9d649ab2cf9d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We can see that the values are now capped at 1, but the distributions are not centered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f272881-56c8-4704-aa94-b94546c37a93",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "====================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3aa91134-7fc0-4f2f-a2e9-d7f5399f3f84",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Last Topic: Dummy variables\n",
    "In some cases we'll have to transform categorical variables into dummy variables in order to use them in machine learning modelling. Dummy variables are also known as binary, because they can assume just two values: 0 or 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bbf9c1c-5144-4eb5-a80b-b448efe22dee",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's use the automobile dataset from the internet to pratice standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb447709-054b-4412-8f2b-62d496d461aa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's use the automobile dataset from the internet.\n",
    "path='https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DA0101EN/automobileEDA.csv'\n",
    "data = pd.read_csv(path)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b50f11aa-df1e-4b03-83aa-cb47fea9c1cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "data_not_categorical = data.select_dtypes(numpy.number)\n",
    "data_not_categorical.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e32d38a5-5d4e-489f-b1d1-395fde52d4ab",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's first select the categorical variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "259431e5-378d-4be9-900e-cb1b79d4729a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "path='https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DA0101EN/automobileEDA.csv'\n",
    "data = pd.read_csv(path)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f23312c-a582-4724-9711-29e2c3accbb6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filtering just categorical variables\n",
    "data_categorical = data.select_dtypes(numpy.object_)\n",
    "data_categorical.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "840defdc-618f-4cce-bac8-3e84eb150917",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's use *get_dummies* function from Pandas to convert each categorical variable in as many 0/1 variables as there are different values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3700f93b-97fb-44db-afe2-8673e35b0d70",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_categorical = pd.get_dummies(data_categorical)\n",
    "data_categorical.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c65d8be7-9883-4d18-bef0-f094ee4324ce",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Now we are ready to start with Machine Learning !!! :)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81f7c7f0-bbe8-492a-93f2-ee35ec0dfbd7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Authors:** Juliana Coelho, Camila Mizokami\n",
    "\n",
    "**Adapted by:** Kamilla Silva\n",
    "\n",
    "**References:**\n",
    "\n",
    "- [About Feature Scaling and Normalization](https://sebastianraschka.com/Articles/2014_about_feature_scaling.html)\n",
    "- [Normalization vs Standardization — Quantitative analysis](https://towardsdatascience.com/normalization-vs-standardization-quantitative-analysis-a91e8a79cebf)\n",
    "- [Get Dummies in Pandas](https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Class_6_part_2 - Student",
   "widgets": {}
  },
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
