{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b73a47b8-691b-411b-ac76-6e8a5e2b30c1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Class 9 - Part 2 \n",
    "### Advanced Machine Learning Concepts\n",
    "In this practical session, we will delve into several advanced machine learning concepts crucial for developing robust predictive models. Here's an overview of the key topics we will explore:\n",
    "\n",
    "1. **Curse of Dimensionality:** We will explore how high-dimensional spaces increase the sparsity of data, making it difficult to effectively train and optimize machine learning models due to increased computational complexity and data requirements.\n",
    "2. **Linear Discriminant Analysis (LDA):** This topic covers LDA, a dimensionality reduction technique that is also used for classification. It seeks to maximize the separability among known categories.\n",
    "3. **Hyperparameter Tuning:** We'll discuss methods to select the best set of hyperparameters for a given machine learning model, enhancing its performance on unseen data.\n",
    "   - **Grid Search:** This is a technique for hyperparameter tuning that methodically builds and evaluates a model for each combination of algorithm parameters specified in a grid.\n",
    "4. **Metrics in Machine Learning:** We will review various metrics used to evaluate the performance of machine learning models, such as accuracy, precision, recall, F1-score, and AUC-ROC among others.\n",
    "   - **Confusion Matrix:** An important tool to summarize the performance of a classification algorithm. It provides insights into the types of errors made by the model.\n",
    "5. **k-Fold Cross-Validation:** This topic covers the technique of dividing the dataset into k-subsets and using each in turn for testing a model trained on the remaining k-1 subsets. It is a method used to estimate the skill of the model on new data.\n",
    "\n",
    "This session will provide hands-on experience in implementing these concepts in Python, using various libraries to help illustrate these advanced techniques in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e13ba69e-1ce9-489c-982e-01b827efcece",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries and modules for data manipulation, machine learning, and metric evaluation:\n",
    "import numpy as np  # Import NumPy, a library for numerical operations like matrix manipulations and advanced mathematical functions.\n",
    "import pandas as pd  # Import pandas, a library for data manipulation and analysis, particularly useful for handling structured data like data frames.\n",
    "import seaborn as sns  # Import seaborn, a Python data visualization library based on matplotlib that provides a high-level interface for drawing attractive statistical graphics.\n",
    "import matplotlib.pyplot as plt  # Import pyplot from matplotlib to create figures and plots, essential for data visualization in Python.\n",
    "\n",
    "# Import specific functions and classes from scikit-learn (sklearn):\n",
    "from sklearn.datasets import load_iris  # Import the function to load the Iris dataset, a classic dataset in pattern recognition.\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold  # Import functions and classes for splitting data into train/test sets, cross-validating scores, and creating stratified folds for cross-validation.\n",
    "from sklearn.neighbors import KNeighborsClassifier  # Import the k-Nearest Neighbors classifier, a simple yet effective classification algorithm.\n",
    "from sklearn.svm import SVC  # Import the Support Vector Machine classifier, powerful for medium-sized datasets and complex problems.\n",
    "from sklearn.naive_bayes import GaussianNB  # Import the Gaussian Naive Bayes classifier, effective for classification based on applying Bayes' theorem with strong (naive) feature independence assumptions.\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, classification_report  # Import various performance metrics to evaluate the accuracy, precision, recall, F1 score, and more comprehensive classification reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "487711aa-17e8-404b-a619-1d44d9d8c615",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the Iris dataset from sklearn's dataset library. The Iris dataset is commonly used for testing machine learning algorithms.\n",
    "iris = load_iris()\n",
    "\n",
    "# Assign the feature data to 'X' and the target labels to 'y'. \n",
    "# 'X' contains the attributes (sepal length, sepal width, petal length, petal width), \n",
    "# while 'y' contains the class labels (species of iris flowers).\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets. \n",
    "# 'test_size=0.3' configures 30% of the dataset to be used as the test set. \n",
    "# 'random_state=22' ensures that the splits are reproducible and consistent across different runs.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc1cb490-859f-4d2f-93c5-923ef52c1784",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Data Description:\n",
    "The Iris dataset is one of the most well-known datasets in the field of machine learning. It was introduced by British statistician and biologist Ronald Fisher in 1936 as an example of linear discriminant analysis. This dataset is often used for classification tasks and testing machine learning algorithms.\n",
    "\n",
    "The Iris dataset consists of 150 instances of iris flowers, each with four attributes: sepal length, sepal width, petal length, and petal width. All measurements are in centimeters. These data are used to classify the instances into one of three species or classes of iris, which are:\n",
    " - Iris-setosa (0)\n",
    " - Iris-versicolor (1)\n",
    " - Iris-virginica(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a737b67d-7bd0-4cc4-943c-c8066b8b6af7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "![Iris](https://www.math.umd.edu/~petersd/666/html/iris_with_labels.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7305100-b07a-40cd-a506-2f346240be34",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define classifiers with their respective hyperparameters:\n",
    "# Initialize the K-Nearest Neighbors (KNN) classifier with 3 neighbors. \n",
    "# This setting specifies that the label of a new point is predicted based on the majority vote of its three nearest neighbors.\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Initialize the Support Vector Machine (SVM) with a linear kernel. \n",
    "# The linear kernel is chosen for its simplicity and effectiveness in linearly separable data sets.\n",
    "svm = SVC(kernel='linear')\n",
    "\n",
    "# Initialize the Gaussian Naive Bayes classifier, \n",
    "# which applies Naive Bayes classification methods assuming that features follow a Gaussian distribution.\n",
    "nb = GaussianNB()\n",
    "\n",
    "# Fit models to the training data:\n",
    "# Train the KNN model using the training data and labels.\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Train the SVM model using the training data and labels. This involves finding the hyperplane that best separates the classes.\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Train the Gaussian Naive Bayes model using the training data and labels. \n",
    "# This model calculates the probabilities of the different features belonging to each class based on the Gaussian distribution.\n",
    "nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6bd8740-ac4c-4ac2-9473-c3cb3ca0550e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate predictions using the fitted models:\n",
    "pred_knn = knn.predict(X_test)  # Use the KNN model to predict the labels for the test data.\n",
    "pred_svm = svm.predict(X_test)  # Use the SVM model to predict the labels for the test data.\n",
    "pred_nb = nb.predict(X_test)    # Use the Naive Bayes model to predict the labels for the test data.\n",
    "\n",
    "# Create confusion matrices for each model to evaluate their performance:\n",
    "cm_knn = confusion_matrix(y_test, pred_knn)  # Confusion matrix for KNN predictions.\n",
    "cm_svm = confusion_matrix(y_test, pred_svm)  # Confusion matrix for SVM predictions.\n",
    "cm_nb = confusion_matrix(y_test, pred_nb)    # Confusion matrix for Naive Bayes predictions.\n",
    "\n",
    "# Display the confusion matrices using seaborn's heatmap for better visualization:\n",
    "fig, ax = plt.subplots(1, 3, figsize=(18, 6))  # Create a figure with three subplots.\n",
    "sns.heatmap(cm_knn, annot=True, ax=ax[0], cmap='Blues', fmt='g')  # Display KNN confusion matrix.\n",
    "ax[0].set_title('KNN Confusion Matrix')  # Set title for the KNN confusion matrix plot.\n",
    "sns.heatmap(cm_svm, annot=True, ax=ax[1], cmap='Blues', fmt='g')  # Display SVM confusion matrix.\n",
    "ax[1].set_title('SVM Confusion Matrix')  # Set title for the SVM confusion matrix plot.\n",
    "sns.heatmap(cm_nb, annot=True, ax=ax[2], cmap='Blues', fmt='g')  # Display Naive Bayes confusion matrix.\n",
    "ax[2].set_title('Naive Bayes Confusion Matrix')  # Set title for the Naive Bayes confusion matrix plot.\n",
    "\n",
    "# Print classification reports to provide a comprehensive overview of the performance of each classifier:\n",
    "print(\"KNN Classification Report:\\n\", classification_report(y_test, pred_knn))  # Print KNN classification report.\n",
    "print(\"SVM Classification Report:\\n\", classification_report(y_test, pred_svm))  # Print SVM classification report.\n",
    "print(\"Naive Bayes Classification Report:\\n\", classification_report(y_test, pred_nb))  # Print Naive Bayes classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28381d28-59dc-486e-a63d-078e5f439b68",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the k-Fold validation:\n",
    "# Create an instance of StratifiedKFold, which ensures that each fold of the dataset has the same proportion of examples in each class,\n",
    "# using 5 splits, which is a common choice for k-fold cross-validation.\n",
    "kfold = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# Calculate cross-validation scores for each classifier:\n",
    "# Compute cross-validation scores for the KNN classifier. This function evaluates the model using the k-fold cross-validation method,\n",
    "# returning the accuracy for each fold, ensuring a robust estimate of the model's performance.\n",
    "scores_knn = cross_val_score(knn, X, y, cv=kfold)\n",
    "\n",
    "# Compute cross-validation scores for the SVM classifier. Similarly, this evaluates the SVM model across each fold defined by StratifiedKFold, providing accuracy scores that help assess the overall effectiveness of the model.\n",
    "scores_svm = cross_val_score(svm, X, y, cv=kfold)\n",
    "\n",
    "# Compute cross-validation scores for the Naive Bayes classifier. This uses the same k-fold cross-validation to evaluate how the GaussianNB model performs on different subsets of the dataset, giving a clear picture of its generalization capability.\n",
    "scores_nb = cross_val_score(nb, X, y, cv=kfold)\n",
    "\n",
    "# Print the cross-validation scores for each model to see their performance stability and effectiveness:\n",
    "print(\"KNN Cross-Validation Scores:\", scores_knn)  # Output the array of scores for KNN from each fold.\n",
    "print(\"SVM Cross-Validation Scores:\", scores_svm)  # Output the array of scores for SVM from each fold.\n",
    "print(\"Naive Bayes Cross-Validation Scores:\", scores_nb)  # Output the array of scores for Naive Bayes from each fold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2edd280f-c30d-459d-9455-5e395931237d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Cross-validation scores are a powerful measure for assessing a model's effectiveness across different segments of data, aiding in robust model selection and tuning, and providing confidence in the model's ability to generalize to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37f5d186-ed19-4661-8d8e-64979320e5c0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "Author: <b>Julio Iglesias</b>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Class_9_Supervised_Learning-Student_Part2",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
