{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9aada49a-70be-498b-b588-4c83e66d0329",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Missing Values\n",
    "\n",
    "Missing data, or missing values, occur when __no data__ / __no value__ is stored for certain observations within a variable. \n",
    "\n",
    "Incomplete data is an unavoidable problem in most data sources, and may have a significant impact on the conclusions that can be derived from the data. \n",
    "\n",
    "### Why is data missing?\n",
    "\n",
    "The source of missing data can be very different. These are just a few examples:\n",
    "\n",
    "- A value is missing because it was forgotten, lost or not stored properly\n",
    "- For a certain observation, the value does not exist\n",
    "- The value can't be known or identified\n",
    "\n",
    "In many organisations, information is collected into a form by a person talking with a client on the phone, or alternatively, by customers filling forms online. Often, the person entering the data does not complete all the fields in the form. Many of the fields are not compulsory, which may lead to missing values.\n",
    "\n",
    "The reasons for omitting the information can vary: perhaps the person does not want to disclose some information, for example income, or they do not know the answer, or the answer is not applicable for a certain circumstance, or on the contrary, the person in the organisation wants to spare the customer some time, and therefore omits asking questions they think are not so relevant.\n",
    "\n",
    "There are other cases where the value for a certain variable does not exist. For example, in the variable 'total debt as percentage of total income' (very common in financial data), if the person has no income, then the total percentage of 0 does not exist, and therefore it will be a missing value.\n",
    "\n",
    "It is important to understand **how the missing data are introduced in the dataset**, that is, the **mechanisms** by which missing information is introduced in a dataset. Depending on the mechanism, we may choose to process the missing values differently. In addition, by knowing the source of missing data, we may choose to take action to control that source and decrease the amount of missing information looking forward during data collection.\n",
    "\n",
    "\n",
    "### Missing Data Mechanisms\n",
    "\n",
    "There are 3 mechanisms that lead to missing data, 2 of them involve missing data randomly or almost-randomly, and the third one involves a systematic loss of data.\n",
    "\n",
    "#### Missing Completely at Random, MCAR:\n",
    "\n",
    "A variable is missing completely at random (MCAR) if the probability of being missing is the same for all the observations. \n",
    "When data is MCAR, there is absolutely no relationship between the data missing and any other values, observed or missing, within the dataset. In other words, those missing data points are a random subset of the data. There is nothing systematic going on that makes some data more likely to be missing than other. If values for observations are missing completely at random, then disregarding those cases would not bias the inferences made.\n",
    "\n",
    "\n",
    "#### Missing at Random, MAR: \n",
    "\n",
    "MAR occurs when there is a relationship between the propensity of missing values and the observed data. In other words, the probability of an observation being missing depends on available information (i.e., other variables in the dataset). For example, mechanical sensors are more likely to fail than electronic ones. For that reason, data imputed from mechanical sensors can be missing more often. \n",
    "\n",
    "In a situation like the above, if we decide to proceed with the variable with missing values, we might benefit from including the type of sensor to control the bias.\n",
    "\n",
    "\n",
    "#### Missing Not at Random, MNAR: \n",
    "\n",
    "Missing data is not at random (MNAR) when there is a mechanism or a reason why missing values are introduced in the dataset. For example, when a financial company asks for bank and identity documents from customers in order to prevent identity fraud, typically, fraudsters impersonating someone else will not upload documents, because they don't have them, because they are fraudsters. Therefore, there is a systematic relationship between the missing documents and the target we want to predict: fraud.\n",
    "\n",
    "Understanding the mechanism by which data is missing is important to decide which methods to use to impute the missing values.\n",
    "\n",
    "===================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13764c1c-4710-4108-a278-2b8f0fd08952",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## In this class, we will:\n",
    "\n",
    "- Learn how to detect and quantify missing values\n",
    "\n",
    "- Try to identify the 3 different mechanisms of missing data introduction\n",
    "\n",
    "We will use the toy Loan dataset and the Titanic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fd849a7-d5a0-4929-9e77-eca6992b0180",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# to display the total number columns present in the dataset\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff9fd741-7ef8-4c9b-ac1c-3479b048bb40",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# let's load the titanic dataset\n",
    "data = pd.read_csv('/dbfs/FileStore/CDS2024/titanic.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6188f47c-03e1-429e-ae07-051b3dc2ddad",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In python, the missing values are stored as NaN, see for example the first row for the variable 'Cabin'.\n",
    "We can quantify the total number of missing values using the **isnull** method plus the sum method on the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d60f6a6e-cfcf-4007-b4f0-4fe31f4f5a1c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26ed9721-feb7-4977-be30-4dac21238367",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "There are 177 missing values for Age, 687 for Cabin and 2 for Embarked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10ae3be8-6ab6-4c88-b819-6f770c67d847",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Alternatively, we can use the mean method after isnull to visualize the percentage of missing values for each variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef5d06ef-f92d-4aa9-9c23-1424b8d35a84",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data.isnull().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b4cc725-b38b-4e8a-b86e-373c02235e6f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "There are missing data in the variables Age (20% missing), Cabin - in which the passenger was traveling - (77% missing), and Embarked - the port from which the passenger got into the Titanic - (~0.2%  missing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aad77496-46d1-4c0e-b4fd-ec1ca2d3be72",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Mechanisms of Missing Data\n",
    "\n",
    "### Missing data Not At Random (MNAR): Systematic missing values\n",
    "\n",
    "In the Titanic dataset, both the missing values of the variables **age** and **cabin** were introduced systematically. For many of the people who did not survive, the **age** they was or the **cabin** they were traveling in, could not be established. The people who survived could be otherwise asked for that information.\n",
    "\n",
    "Can we infer this by looking at the data?\n",
    "\n",
    "In a situation like this, we could expect a greater number of missing values for people who did not survive.\n",
    "\n",
    "Let's have a look."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3425dcec-3812-4e74-8472-83612ca0dc53",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's create a binary variable that indicates whether the value of cabin is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df58d1ab-a137-4421-a5eb-cc18cc2025d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data['cabin_null'] = np.where(data['Cabin'].isnull(), 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f422d1b-0a4d-44e8-8f3a-3966552f693c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's evaluate the percentage of missing values in cabin for the people who survived vs the non-survivors.\n",
    "\n",
    "The variable Survived takes the value 1 if the passenger survived, or 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68318f41-f17f-4eb0-880c-8ac0c7ed73be",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Group data by Survived vs Non-Survived and find the percentage of nulls for cabin\n",
    "data.groupby(['Survived'])['cabin_null'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1200a041-0ce6-4200-a34d-92afe07970b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# another way of doing the above\n",
    "\n",
    "data['Cabin'].isnull().groupby(data['Survived']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd4f1615-5e32-44c6-a109-4d201bcc8dac",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We observe that the percentage of missing values is higher for people who did not survive (87%), respect to people who survived (60%). This finding is aligned with our hypothesis that the data is missing because after people died, the information could not be retrieved.\n",
    "\n",
    "**Note**: Having said this, to truly underpin whether the data is missing not at random, we would need to get extremely familiar with the way data was collected. Analyzing datasets, can only point us in the right direction or help us build assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bbb4c9f-76c0-4ed5-a916-563d3c6b657f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Excercise:** Let's do the same for the variable Age. What can we conclude?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b49e9b2-5a1a-42ee-ad21-87b97cd338bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# First we create a binary variable to indicates whether the value of Age is missing\n",
    "\n",
    "# Then look at the mean in the different survival groups:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58c577cd-8da8-40aa-82bf-dd6bc8b7e720",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Conclusion:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e983f63-ff1a-4f13-86f9-2ce3778b66bb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Missing data Completely At Random (MCAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bb56347-62d5-4aeb-a9e6-f4daef14545c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In the titanic dataset, there are also missing values for the variable Embarked. Let's have a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18bfe4f0-979e-44f7-98f9-d1c9dd5a36e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data[data['Embarked'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "932e2c5a-bf1b-4fd1-bf96-2637eaf172d6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "These 2 women were traveling together, Miss Icard was the maid of Mrs Stone.\n",
    "\n",
    "A priori, it does not seem to be an indication that the missing information in the variable Embarked is depending on any other variable, and the fact that these women survived means that they could have been asked for this information.\n",
    "\n",
    "Very likely the values were lost at the time of building the dataset.\n",
    "\n",
    "If these values are MCAR, the probability of data being missing for these 2 women is the same as the probability for values to missing for any other person on the titanic. Of course this will be hard, if possible at all, to prove. But I hope this serves as a demonstration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc37f1f8-f9cb-4d1e-94a6-b29f99c49dee",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Missing data at Random (MAR)\n",
    "\n",
    "For this example, I will use the loan book toy dataset from this ficticious peer to peer lending company.\n",
    "\n",
    "We will look at the variables employment and years in employment, both declared by the borrowers at the time of applying for a loan. \n",
    "\n",
    "In this example, data missing in employment are associated with data missing in time in employment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6fc4161-9882-42d2-8777-9db2001defa0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's use the Loan dataset and load the columns of interest for this exercise\n",
    "data = pd.read_csv('/dbfs/FileStore/CDS2024/loan.csv', usecols=['employment', 'time_employed'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfa7ee83-d41b-4513-9851-46abed013a8c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# let's check the percentage of missing data\n",
    "\n",
    "data.isnull().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95e6a1d5-0c4d-4ca1-9ee2-8eb07f5e7a07",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We see that both variables have almost the same percentage of missing observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "762270e1-a6ef-49b4-8a92-fc3123919175",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's inspect the different employment types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4716a1ab-4969-4633-b73a-d4d8c39c63bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# number of different employments\n",
    "n_employments = len(data['employment'].unique())\n",
    "print('Number of employments: ' + str(n_employments))\n",
    "\n",
    "# Examples of employments\n",
    "data['employment'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c54d7f3a-b849-4d9d-920e-052a2adadc4b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We observe the missing information (nan), and several different employments of the people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c682ba80-06a7-42fe-9e82-979a2604a9d1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's inspect the variable time employed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33a28d32-d4d2-4a9d-a377-b031d1a8bb4a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data['time_employed'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebf4986b-c2c3-4b9a-b1fc-862393da2790",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The customer can't enter a value for employment time if they are not employed. They could be students, retired, self-employed, or work in the house. But we can see how these 2 variables are related to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a41aa48-ab59-44da-b44a-ca0a8e3ff4d0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's calculate the proportion of missing data for the time_employed variable for customers who declared employment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "603cc886-be57-4910-9203-0e3c5ac3936e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# customers who declared employment\n",
    "t = data[~data['employment'].isnull()]\n",
    "\n",
    "# percentage of missing data in time employed\n",
    "print(t['time_employed'].isnull().mean(), '->' , round(t['time_employed'].isnull().mean()*100, 2), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d26f705-6688-4627-87eb-266f68021659",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Exercise:** Let´s do the same for those who did not report employment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69c7fe67-bf5d-4ee9-a5ce-707e10f39ec6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# customers who did not declare employment\n",
    "\n",
    "# percentage of missing data in time employed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "194871e1-2c46-4827-bdac-4159f2f0ce79",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Conclusion:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d01014be-b14d-4df5-843e-4417145e9609",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Now let's see how we can deal with missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c352ded8-68b2-44d7-aaa3-025af8bab84e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Complete Case Analysis\n",
    "\n",
    "\n",
    "Complete-case analysis (CCA), also called \"list-wise deletion\" of cases, consists in **discarding** observations where values in **any** of the variables are missing. Complete Case Analysis means literally analysing only those observations for which there is information in **all** of the variables in the dataset. \n",
    "\n",
    "### Which variables can I impute with CCA?\n",
    "\n",
    "CCA can be applied to both categorical and numerical variables.\n",
    "\n",
    "\n",
    "### Assumptions\n",
    "\n",
    "CCA works well when the data are missing completely at random (MCAR). In fact, we should use CCA only if we have reasons to believe that data is missing at random, and not otherwise. When data is MCAR, excluding observations with missing information is in essence the same as randomly excluding some observations from the dataset. Therefore the dataset after CCA is a fair representation of the original dataset. \n",
    "\n",
    "\n",
    "### Advantages\n",
    "\n",
    "- Easy to implement\n",
    "- No data manipulation required\n",
    "- Preserves variable distribution (if data is MCAR, then the distribution of the variables of the reduced dataset should match the distribution in the original dataset)\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "- It can exclude a large fraction of the original dataset (if missing data is abundant)\n",
    "- Excluded observations could be informative for the analysis (if data is not missing at random)\n",
    "- CCA will create a biased dataset if the complete cases differ from the original data (e.g., when missing information is in fact MAR or NMAR and not missing at random).\n",
    "- When using our models in production, the model will not know how to handle missing data\n",
    "\n",
    "### When to use CCA\n",
    "\n",
    "- Data is missing completely at random\n",
    "- No more than 5% of the total dataset contains missing data\n",
    "\n",
    "In practice, CCA may be an acceptable method when the amount of missing information is small. Unfortunately, there is no rule of thumb to determine how much missing data is small or negligible. However, as general guidance, if the total amount of missing data is ~5% of the original dataset or less, CCA is a viable option.\n",
    "\n",
    "In many real life datasets, the amount of missing data is never small, and therefore CCA is typically never an option.\n",
    "\n",
    "### CCA and models in production\n",
    "\n",
    "When using CCA, we remove all observations that contain missing information. However, the data that we want to score with our model, may indeed contain missing information. This will pose a problem when using our model in live systems, or as we call it, when putting or models into production: when an observation contains missing data, the model will not be able to handle it. \n",
    "\n",
    "In order to avoid this problem, when putting models into production we need to do 1 of 2 things: either we do not score observations with missing data, or we replace the missing values by another number. We can choose any from the imputation techniques that we will discuss in the following lectures to replace NA in the data to be scored.\n",
    "\n",
    "## In this section:\n",
    "\n",
    "We will use the House Prices dataset to demonstrate how to perform Complete Case Analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3620fa6-97eb-46de-a1cd-f8d4f6f8d4ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# let's load the House Prices dataset and explore its shape (rows and columns)\n",
    "data = pd.read_csv('/dbfs/FileStore/CDS2024/houseprice.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5be7ee1a-9a4c-4ec8-aa0c-d5fefddb1986",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7827d2bf-ba35-4215-8588-8aa2881aba79",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# to show all the columns of the dataframe in the notebook\n",
    "pd.set_option('display.max_columns', None)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3eceafad-0529-4be0-9a44-bbf60c75a7d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We can calculate the percentage of missing values ('NA' - Not available) for each variable and select those with more than 0%.\n",
    "\n",
    "vars_with_na = [var for var in data.columns if data[var].isnull().mean() > 0]\n",
    "vars_with_na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "716492a0-b617-4a12-95fb-391d044db573",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# let's find out whether they are numerical or categorical\n",
    "data[vars_with_na].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7928319a-35b7-4d60-867c-aa70d14f6a36",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "There are both numerical and categorical variables with missing observations. We can see from the variable types that some are float and some are object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "704f7958-7ff0-4e45-ae3d-6015073c9c0d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# let's have a look at the values of the variables with missing data\n",
    "\n",
    "data[vars_with_na].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "031a9f3b-8f9a-4143-9444-718c1e3366e1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Let's find out the percentage of observations missing per variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6e0cc50-6f67-480c-b904-6d57575f0432",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "1 - Calculating the percentage of missing  using the isnull() and mean() methods from pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab5dc12d-4285-45c7-9c7e-552fa0468466",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_na = data[vars_with_na].isnull().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ed3263b-7eb7-4035-a22d-8d3698d0b36b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "2 - Transforming the array into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e271368-75d2-4c77-9809-370f5246d416",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_na = pd.DataFrame(data_na.reset_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0254bd78-ece0-47fc-8d3c-6598ffb897ab",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "3 - Adding column names to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "724aca8e-0293-4640-bd6e-dadbd77119a2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_na.columns = ['variable', 'na_percentage']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eafe3ac4-7a34-4b3d-b08d-5f5f48b2999b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "4 - Ordering the dataframe according to percentage of na per variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "553752d7-3f7c-4ff3-b248-200fc06e851a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_na.sort_values(by='na_percentage', ascending=False, inplace=True)\n",
    "\n",
    "data_na"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bb920b6-98e6-43c7-b663-304f72e457d4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The first 6 variables contain a lot of missing information. So we can't use CCA if we consider those variables, as most of the observations in the dataset will be discarded. We could otherwise use CCA if we omit using those variables with a lot of NA.\n",
    "\n",
    "For this demo, I will ignore the first 6 variables with a lot of missing data, and proceed with CCA in the remaining of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dbbdc53-4d8a-4788-9e0f-f23af195de25",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Selecting variables with no or less than 5% NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb95c7ea-abd9-4ae3-ab05-6b63cc504e41",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "vars_cca = [var for var in data.columns if data[var].isnull().mean() < 0.05]\n",
    "vars_cca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b16baf41-1849-49fb-b280-061f19cfff77",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data.shape[1], len(vars_cca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0910c3d7-9733-4a6e-a548-60023d4b18a8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Calculating the percentage of observations with complete cases: i.e., with values for all the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e60c542e-6106-4ffe-a8a2-d1328d49690f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# The method dropna(), discards the observations that contain na in any of the rows / columns\n",
    "\n",
    "len(data[vars_cca].dropna()) / len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "598f8991-3233-4c03-a9bc-d2c972efc4aa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Exercise:** Let's create the complete case dataset. In other words, remove observations with NA in any variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9510fa86-2770-4e92-a84b-96d8dc3d3b49",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# New data set will consider the columns in vars_cca and then drop na \n",
    "data_cca = data[vars_cca].dropna()\n",
    "\n",
    "# .shape to check the size of both data sets\n",
    "data.shape, data_cca.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64b6afb7-0a27-4302-b226-caa45967a0fe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's plot the histograms for all numerical variables in the complete case dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1db60d82-4665-48c0-a133-c1b47aef4300",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_cca.hist(bins=50, density=True ,figsize=(16, 16))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "987410e2-e217-4c29-9b9d-c6a1d37bdfcf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's check the distribution and the density of a few variables before and after CCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9b775c9-dc3f-40a7-8572-1424e2397549",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# original data\n",
    "data['GrLivArea'].hist(bins=50, ax=ax, density=True, color='red') # density = True: normalize variable\n",
    "\n",
    "# data after cca, the argument alpha makes the color transparent, so we can see the overlay of the 2 distributions\n",
    "data_cca['GrLivArea'].hist(bins=50, ax=ax, color='blue', density=True, alpha=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "223618aa-1e85-4378-982f-2879804b54c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# original data\n",
    "data['GrLivArea'].plot.density(color='red')\n",
    "\n",
    "# data after cca\n",
    "data_cca['GrLivArea'].plot.density(color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7cba559-f05e-4b0b-b7f1-ee9bd5eb5213",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# original data\n",
    "data['BsmtFinSF1'].hist(bins=50, ax=ax, density=True, color='red')\n",
    "\n",
    "# data after cca\n",
    "data_cca['BsmtFinSF1'].hist(bins=50, ax=ax, color='blue', density=True, alpha=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78941649-e1b9-4770-9525-8037cac87412",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# original data\n",
    "data['BsmtFinSF1'].plot.density(color='red')\n",
    "\n",
    "# data after cca\n",
    "data_cca['BsmtFinSF1'].plot.density(color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15657e03-3e36-482f-8727-ef7814f9e8b7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "As we can see from the above plots, the distribution of the selected numerical variables in the original and complete case dataset is very similar, which is what we expect from CCA if data is missing at random and only for a small proportion of the observations.\n",
    "\n",
    "In the next cells I will explore the distribution of categorical variables. To do so, I will evaluate the percentage of observations that show each of the unique categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbf510e7-7387-4235-b41d-b8a3e2caaff9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#The following function captures the percentage of observations for each category in the original and complete case dataset\n",
    "# and puts them together in a new dataframe\n",
    "\n",
    "\n",
    "def categorical_distribution(df, df_cca, variable):\n",
    "    tmp = pd.concat(\n",
    "        [\n",
    "            # percentage of observations per category, original data\n",
    "            df[variable].value_counts() / len(df),\n",
    "\n",
    "            # percentage of observations per category, cca data\n",
    "            df_cca[variable].value_counts() / len(df_cca)\n",
    "        ],\n",
    "        axis=1)\n",
    "\n",
    "    # add column names\n",
    "    tmp.columns = ['original', 'cca']\n",
    "\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a654d2c-d892-431c-b896-3704f7e2d0d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# run the function in a categorical variable\n",
    "categorical_distribution(data, data_cca, 'BsmtQual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "526781a0-39d3-43b0-b080-7f5b46dc8bfa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "categorical_distribution(data, data_cca, 'MasVnrType')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b21a606a-58b2-4258-a17f-7438e45c84c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "categorical_distribution(data, data_cca, 'SaleCondition')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e34625ca-bac1-4c3d-b07d-60e76d14e146",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "As we can see from the output of the cells above, the distribution of houses in each of the categories, is very similar in the original and complete case dataset, which again, is what is expected if the data is missing completely at random, and the percentage of missing data is small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c318203-2ded-4b5e-b909-0a16167378bc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Now let's see another method to deal with missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76fbfc77-9c0e-40df-b87e-a98fd808fcee",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Mean / Median imputation\n",
    "\n",
    "Imputation is the act of replacing missing data with statistical estimates of the missing values. The goal of any imputation technique is to produce a **complete dataset** that can be used to train machine learning models.\n",
    "\n",
    "Mean / median imputation consists of replacing all occurrences of missing values (NA) within a variable by the mean (if the variable has a Gaussian distribution) or median (if the variable has a skewed distribution).\n",
    "\n",
    "**Note the following**:\n",
    "\n",
    "- If a variable is normally distributed, the mean, median and mode, are approximately the same. Therefore, replacing missing values by the mean and the median are equivalent. Replacing missing data by the mode is not common practice for  numerical variables.\n",
    "- If the variable is skewed, the mean is biased by the values at the far end of the distribution. Therefore, the median is a better representation of the majority of the values in the variable.\n",
    "- For discrete variables casted as 'int' (to save memory), the mean may not be an integer, therefore the whole variable will be re-casted as 'float'. In order to avoid this behaviour, we can replace NA with the median instead. The median will inevitably be an integer / discrete value as well.\n",
    "\n",
    "\n",
    "### Which variables can I impute with Mean / Median Imputation?\n",
    "\n",
    "The mean and median can only be calculated on numerical variables, therefore these methods are suitable for continuous and discrete numerical variables only.\n",
    "\n",
    "\n",
    "### Assumptions\n",
    "\n",
    "- Data is missing completely at random (MCAR)\n",
    "- The missing observations, most likely look like the majority of the observations in the variable (aka, the mean / median)\n",
    "\n",
    "If data is missing completely at random, then it is fair to assume that the missing values, are most likely very close to the value of the mean or the median of the distribution, as these represent the most frequent / average observation.\n",
    "\n",
    "\n",
    "### Advantages\n",
    "\n",
    "- Easy to implement\n",
    "- Fast way of obtaining complete datasets\n",
    "- Can be integrated in production (during model deployment)\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- Distortion of the original variable distribution\n",
    "- Distortion of the original variance\n",
    "- Distortion of the covariance with the remaining variables of the dataset\n",
    "\n",
    "When replacing NA with the mean or median, the variance of the variable will be distorted if the number of NA is big respect to the total number of observations, leading to underestimation of the variance.\n",
    "\n",
    "In addition, estimates of covariance and correlations with other variables in the dataset may also be affected. Mean / median imputation may alter intrinsic correlations since the mean / median value that now replaces the missing data will not necessarily preserve the relation with the remaining variables.\n",
    "\n",
    "Finally, concentrating all missing values at the mean / median value, may lead to observations that are common occurrences in the distribution, to be picked up as outliers.\n",
    "\n",
    "\n",
    "### When to use mean / median imputation?\n",
    "\n",
    "- Data is missing completely at random\n",
    "- No more than 5% of the variable contains missing data\n",
    "\n",
    "Although in theory, the above conditions should be met to minimise the impact of this imputation technique, in practice, mean / median imputation is very commonly used, even in those cases when data is not MCAR and there are a lot of missing values. The reason behind this, is the simplicity of the technique.\n",
    "\n",
    "\n",
    "### Final note\n",
    "\n",
    "Replacement of NA with mean / median is widely used in the data science community and in various data science competitions. See for example the winning solution of the KDD 2009 cup: [\"Winning the KDD Cup Orange Challenge with Ensemble Selection\"]( http://www.mtome.com/Publications/CiML/CiML-v3-book.pdf).\n",
    "\n",
    "Typically, mean / median imputation is done together with adding a binary \"missing indicator\" variable to capture those observations where the data was missing (see lecture \"Missing Indicator\"), thus covering 2 angles: if the data was missing completely at random, this would be captured by the mean /median imputation, and if it wasn't this would be captured by the additional \"missing indicator\" variable. Both methods are extremely straight forward to implement, and therefore are a top choice in data science competitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f984d79-ed67-4314-b203-643d8e2f182b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# to split the datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5a5c4da-0170-46d3-bf77-54cf45e34751",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Let's put into practice with Titanic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61c74ce1-ade2-46d6-a237-6567ee048399",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# load the Titanic Dataset with a few variables for demonstration\n",
    "\n",
    "data = pd.read_csv('/dbfs/FileStore/CDS2024/titanic.csv', usecols=['Age', 'Fare', 'Survived'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d75d666-51cb-4a6f-ae85-b4606da03287",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# let's look at the percentage of NA\n",
    "data.isnull().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd13b0d3-d1f1-4109-92f9-7c312a167460",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The only variable with missing data is Age, with ~20% of missing observations.\n",
    "\n",
    "### Imputation - important\n",
    "\n",
    "Imputation should be done over the training set, and then propagated to the test set. This means that the mean / median to be used to fill missing values both in train and test set, should be extracted from the train set only. And this is to avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2862c0d-0da7-47e4-9b53-e88759d82d35",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "First, let's separate the data into training and testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "542a4d50-660f-486a-bb10-0418a9ca457c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data[['Age', 'Fare']],  # predictors\n",
    "    data['Survived'],  # target\n",
    "    test_size=0.3,  # percentage of obs in test set\n",
    "    random_state=0)  # seed to ensure reproducibility\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e901d465-4503-42fc-a402-bf5a34646b85",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's explore the missing data in the train set. The percentages should be fairly similar to those of the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e719367-de71-411c-a789-708ea7de7f2b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_train.isnull().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f4ec4f9-4b55-490f-a235-4b9728843dc2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's make a function to fill missing values with the mean or median:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "838526e6-7fdc-4c55-8f84-8338adfabdbb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# The function takes the dataframe, the variable, and the value of the mean or median as parameters and returns the variable with the filled na.\n",
    "\n",
    "\n",
    "def impute_na(df, variable, mean_median):\n",
    "\n",
    "    return df[variable].fillna(mean_median)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12c9a3fc-86c7-4efc-b22f-c266f010bc7b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Exercise:** What is the mean and median age?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a91bf7f3-357f-4a97-8ffc-88ed34b2295e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# calculate the median Age in X train\n",
    "\n",
    "\n",
    "# calculate the mean Age in X trai\n",
    "\n",
    "\n",
    "# print the information\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d4a890b-2c62-4430-9502-b30577ddd4bf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's create a new variable with the missing values replaced, using the function we created above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d294cbe-47e9-4ab1-be34-034b08db80ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# first replace with the median\n",
    "X_train['Age_median'] = impute_na(X_train, 'Age', median)\n",
    "\n",
    "# now replace with the mean\n",
    "X_train['Age_mean'] = impute_na(X_train, 'Age', mean)\n",
    "\n",
    "# the mean contains many decimals, so I round to 1, using # the round function from numpy\n",
    "X_train['Age_mean'] = np.round(X_train['Age_mean'], 0)\n",
    "X_train.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ebf0359-f605-4c22-a394-6a8635c80530",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Look at the rows with missing data (NaN) in Age, and see how in the new variables those were replaced by either 29 (median) or 30 (mean)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acd7bced-6096-47d7-ba5a-dbd6de27871c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's see if there is a variance change after the imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcee47cc-b3a1-496d-b76e-b549a257e56a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print('Original variable variance: ', X_train['Age'].var())\n",
    "print('Variance after median imputation: ', X_train['Age_median'].var())\n",
    "print('Variance after mean imputation: ', X_train['Age_mean'].var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0ac8dc5-b484-48ae-b980-696d08fdea22",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We can see a change in the variance after mean / median imputation. This is expected, because the percentage of missing data is quite high in Age, ~20%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1c3219f-cdb7-4051-b3a0-2777d31a6be1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "As expected, the variance is underestimated, because now many values are the same ==> either the mean or the median value. We can see through graphs that the distribution has changed, accumulating more values towards the median or median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d725b887-3f39-4d84-80b2-0c950d5c9f85",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# original variable distribution\n",
    "X_train['Age'].plot(kind='kde', ax=ax)\n",
    "\n",
    "# variable imputed with the median\n",
    "X_train['Age_median'].plot(kind='kde', ax=ax, color='red')\n",
    "\n",
    "# variable imputed with the mean\n",
    "X_train['Age_mean'].plot(kind='kde', ax=ax, color='green')\n",
    "\n",
    "# add legends\n",
    "lines, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(lines, labels, loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e69eaf6-b9cf-486d-a8d7-87cd1d0cc183",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "As mentioned above, the mean / median imputation distorts the original distribution of the variable Age. The transformed variable shows more values around the mean / median values.\n",
    "\n",
    "**Is this important?**\n",
    "\n",
    "It depends on the machine learning model you want to build. Linear models assume that the variables are normally distributed. Mean / median imputation may distort the original normal distribution if the % of missing data is high. Therefore the final imputed variable will no longer be normally distributed, which in turn may affect the linear model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8eae4cab-c983-40d4-8b9d-ef7c801af0e5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Finally, we mentioned that mean / median imputation may lead to observations that are normal, to look like outliers or in other words, mean / median imputation may lead to an increase in the apparent number of  outliers. Let's use a boxplot to see if it happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "309d468b-012f-4397-a57c-4c76e0d338d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_train[['Age', 'Age_median', 'Age_mean']].boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7e62c6f-8d73-4d6a-8d95-7e5025457441",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "From the boxplot above, we can see that after the imputation not only we have more outliers on the higher Age values, but we have now outliers as well for the lower values of Age.\n",
    "\n",
    "**Is this important?**\n",
    "\n",
    "If we are after true outliers, we need to keep this behaviour in mind, to make sure that we are neither masking nor creating artificial outliers with our imputation technique. In practice, we normally don't check for this behaviour at all. But I think it is important to know that is happening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6faaaed-5d7b-476d-8618-a0c259f3d414",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Authors:** Juliana da Mota Coelho, Camila Mizokami. \n",
    "\n",
    "**Adapted by** Kamilla Silva\n",
    "\n",
    "**References:**\n",
    "\n",
    "https://pandas.pydata.org/docs/user_guide/missing_data.html \n",
    "\n",
    "https://chartio.com/resources/tutorials/how-to-check-if-any-value-is-nan-in-a-pandas-dataframe/ \n",
    "\n",
    "https://jakevdp.github.io/PythonDataScienceHandbook/03.04-missing-values.html \n",
    "\n",
    "https://towardsdatascience.com/3-ultimate-ways-to-deal-with-missing-values-in-python-ac5a17c53787"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Class 4 - Missing_Data - student",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
